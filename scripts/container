#!/usr/bin/env python3

"""
Container Management CLI for dev-containers framework
Usage: container <command> [args]
"""

import os
import sys
import subprocess
import argparse
import pwd
from pathlib import Path
from typing import Optional, Dict
import yaml


class ContainerManager:
    def __init__(self):
        self.script_dir = Path(__file__).parent.resolve()
        self.framework_root = self.script_dir.parent
        self.project_root = self.framework_root.parent
        self.config_dir = self.project_root / "container-config"
        self.templates_dir = self.framework_root / "templates"

    def load_environment_config(self, env_name: str, instance_name: Optional[str] = None) -> Dict[str, str]:
        """Load environment configuration from config.env and shared/config.env.
        If instance_name is provided, override container/image names for a named instance."""
        env_dir = self.config_dir / "environments" / env_name

        if not env_dir.exists():
            print(f"Error: Environment '{env_name}' not found")
            sys.exit(1)

        config = {}

        # Load shared config first
        shared_config = self.config_dir / "shared" / "config.env"
        if shared_config.exists():
            config.update(self._parse_env_file(shared_config))

        # Load environment-specific config (overrides shared)
        env_config = env_dir / "config.env"
        if env_config.exists():
            config.update(self._parse_env_file(env_config))
        else:
            print(f"Error: config.env not found in {env_dir}")
            sys.exit(1)

        # Set defaults
        config.setdefault('CONTAINER_NAME', f"{env_name}-container")
        config.setdefault('VOLUME_NAME', f"{env_name}-vol")
        config.setdefault('IMAGE_NAME', f"{env_name}-image")
        config.setdefault('CPU_LIMIT', '8.0')
        config.setdefault('MEMORY_LIMIT', '16G')
        config.setdefault('PRIVILEGED', 'false')

        # Override names for named instances
        if instance_name:
            config['CONTAINER_NAME'] = f"{instance_name}-container"
            config['IMAGE_NAME'] = f"{instance_name}-image"

        return config

    def _parse_env_file(self, filepath: Path) -> Dict[str, str]:
        """Parse a .env file and return key-value pairs"""
        config = {}
        with open(filepath) as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    # Remove quotes if present
                    value = value.strip().strip('"').strip("'")
                    config[key.strip()] = value
        return config

    def generate_compose_file(self, env_name: str, config: Dict[str, str], instance_name: Optional[str] = None) -> Path:
        """Generate docker-compose.yml from template with substitutions and merge overrides"""
        file_label = instance_name or env_name
        template_path = self.templates_dir / "docker-compose.template.yml"
        output_path = self.project_root / f".docker-compose.{file_label}.yml"

        with open(template_path) as f:
            content = f.read()

        # Substitute environment variables
        # Handle ${VAR:-default} syntax first - replace with just the value if we have it
        import re
        for key, value in config.items():
            # Replace ${VAR:-default} with the value (removing the default part entirely)
            content = re.sub(rf'\${{{re.escape(key)}:-[^}}]+}}', value, content)
            # Replace plain ${VAR} with the value
            content = content.replace(f"${{{key}}}", value)

        # Load as YAML to merge with override if it exists
        compose_data = yaml.safe_load(content)

        # Check for compose override file
        env_dir = self.config_dir / "environments" / env_name
        override_path = env_dir / "compose.override.yml"

        if override_path.exists():
            with open(override_path) as f:
                override_content = f.read()

            # Apply same variable substitution to overrides
            for key, value in config.items():
                override_content = re.sub(rf'\${{{re.escape(key)}:-[^}}]+}}', value, override_content)
                override_content = override_content.replace(f"${{{key}}}", value)

            override_data = yaml.safe_load(override_content)

            # Deep merge override into compose_data
            compose_data = self._deep_merge(compose_data, override_data)

        # Write final compose file
        with open(output_path, 'w') as f:
            yaml.dump(compose_data, f, default_flow_style=False, sort_keys=False)

        return output_path

    def _deep_merge(self, base: dict, override: dict) -> dict:
        """Deep merge two dictionaries, with override taking precedence.
        Lists are concatenated (with deduplication) rather than replaced."""
        result = base.copy()
        for key, value in override.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = self._deep_merge(result[key], value)
            elif key in result and isinstance(result[key], list) and isinstance(value, list):
                combined = list(result[key])
                for item in value:
                    if item not in combined:
                        combined.append(item)
                result[key] = combined
            else:
                result[key] = value
        return result

    def get_dockerfile_path(self, env_name: str, instance_name: Optional[str] = None) -> str:
        """Get or generate the Dockerfile path for the environment"""
        file_label = instance_name or env_name
        env_dir = self.config_dir / "environments" / env_name
        custom_dockerfile = env_dir / "Dockerfile.custom"
        post_dockerfile = env_dir / "Dockerfile.post"

        has_custom = custom_dockerfile.exists()
        has_post = post_dockerfile.exists()

        if has_custom:
            # Check if this is a partial dockerfile (just custom commands)
            with open(custom_dockerfile) as f:
                content = f.read()

            # If it has a FROM statement, it's a complete Dockerfile
            if 'FROM' in content:
                # Complete Dockerfile - use as-is (Dockerfile.post is ignored)
                return str(custom_dockerfile.relative_to(self.project_root))

        # If either partial custom or post file exists, generate a merged Dockerfile
        if has_custom or has_post:
            merged_dockerfile = self.project_root / f".Dockerfile.{file_label}"
            self._generate_merged_dockerfile(
                env_name,
                custom_file=custom_dockerfile if has_custom else None,
                post_file=post_dockerfile if has_post else None,
                output_file=merged_dockerfile,
            )
            return str(merged_dockerfile.relative_to(self.project_root))
        else:
            return str(self.templates_dir.relative_to(self.project_root) / "Dockerfile.template")

    def _generate_merged_dockerfile(self, env_name: str, custom_file: Optional[Path], post_file: Optional[Path], output_file: Path):
        """Merge custom Dockerfile commands into the template"""
        template_path = self.templates_dir / "Dockerfile.template"

        with open(template_path) as f:
            template_content = f.read()

        custom_content = ""
        if custom_file:
            with open(custom_file) as f:
                custom_content = f.read()

        post_content = ""
        if post_file:
            with open(post_file) as f:
                post_content = f.read()

        merged = template_content

        # Insert pre-template custom commands (Dockerfile.custom)
        if custom_content:
            # Insert custom content between the CUSTOM COMMANDS block and the USER switch.
            # Custom commands run as root (before USER is set), so they can use apt directly.
            user_switch_marker = "# Switch to non-root user for all tool installations"

            parts = merged.split(user_switch_marker)

            if len(parts) == 2:
                merged = (
                    parts[0] +
                    custom_content + "\n\n" +
                    user_switch_marker +
                    parts[1]
                )
            else:
                # Fallback: append before the SSH section
                insertion_marker = "# ============================================================================="
                ssh_section_marker = "# SSH & Basic Env"
                parts = merged.split(f"{insertion_marker}\n{ssh_section_marker}")
                if len(parts) == 2:
                    merged = (
                        parts[0] +
                        custom_content + "\n\n" +
                        insertion_marker + "\n" + ssh_section_marker +
                        parts[1]
                    )
                else:
                    merged = merged.rstrip() + "\n\n" + custom_content + "\n"

        # Append post-template commands (Dockerfile.post)
        if post_content:
            post_marker = "# POST-TEMPLATE COMMANDS INSERTION POINT"
            insertion_marker = "# ============================================================================="

            parts = merged.split(f"{insertion_marker}\n{post_marker}")

            if len(parts) == 2:
                # Find the end of the comment block after the marker
                remaining = parts[1]
                # The comment block ends after the closing separator line
                closing_sep = remaining.find(f"{insertion_marker}")
                if closing_sep != -1:
                    after_block = remaining[closing_sep + len(insertion_marker):]
                    merged = (
                        parts[0] +
                        insertion_marker + "\n" + post_marker +
                        remaining[:closing_sep + len(insertion_marker)] +
                        "\n\n" + post_content +
                        after_block
                    )
                else:
                    merged = merged.rstrip() + "\n\n" + post_content + "\n"
            else:
                merged = merged.rstrip() + "\n\n" + post_content + "\n"

        with open(output_file, 'w') as f:
            f.write(merged)

    def _get_dotfiles_commit(self) -> str:
        """Get the latest commit hash of the dotfiles repo for cache busting."""
        try:
            result = subprocess.run(
                ["git", "ls-remote", "git@github.com:mdavis36/configs.git", "HEAD"],
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0 and result.stdout.strip():
                return result.stdout.split()[0]
        except Exception:
            pass
        # Fallback to timestamp if we can't reach the repo
        return str(subprocess.check_output(["date", "+%s"], text=True).strip())

    def container_is_running(self, container_name: str) -> bool:
        """Check if container is running"""
        result = subprocess.run(
            ["docker", "ps", "-q", "-f", f"name={container_name}"],
            capture_output=True,
            text=True
        )
        return bool(result.stdout.strip())

    def container_exists(self, container_name: str) -> bool:
        """Check if container exists (running or stopped)"""
        result = subprocess.run(
            ["docker", "ps", "-aq", "-f", f"name={container_name}"],
            capture_output=True,
            text=True
        )
        return bool(result.stdout.strip())

    def cmd_list(self):
        """List all available container environments"""
        print("Available container environments:\n")

        env_dir = self.config_dir / "environments"
        if not env_dir.exists():
            print(f"No environments found in {env_dir}")
            return

        for env_path in sorted(env_dir.iterdir()):
            if env_path.is_dir():
                env_name = env_path.name
                config_file = env_path / "config.env"

                if config_file.exists():
                    config = self.load_environment_config(env_name)

                    # Check container status
                    container_name = config.get('CONTAINER_NAME', f"{env_name}-container")
                    status = "running" if self.container_is_running(container_name) else "stopped"

                    print(f"  {env_name}")
                    print(f"    Base Image: {config.get('BASE_IMG', 'N/A')}")
                    print(f"    Container:  {container_name} [{status}]")

                    # Find named instances by looking for running/existing containers
                    # that match the pattern <name>-container where the env config was used
                    self._list_named_instances(env_name)

                    print()

    def _list_named_instances(self, env_name: str):
        """List any named instances spawned from this environment"""
        # Look for generated compose files matching this environment's instances
        for compose_file in sorted(self.project_root.glob(f".docker-compose.*.yml")):
            instance_label = compose_file.stem.replace(".docker-compose.", "")
            if instance_label == env_name:
                continue  # Skip the base environment
            # Check if this compose file references the same environment by reading it
            try:
                with open(compose_file) as f:
                    data = yaml.safe_load(f)
                service = data.get('services', {}).get('dev-env', {})
                cname = service.get('container_name', '')
                if cname == f"{instance_label}-container":
                    status = "running" if self.container_is_running(cname) else "stopped"
                    print(f"    Instance:   {instance_label} ({cname}) [{status}]")
            except Exception:
                pass

    def cmd_activate(self, env_name: str, instance_name: Optional[str] = None):
        """Start and attach to a container environment"""
        config = self.load_environment_config(env_name, instance_name)
        container_name = config['CONTAINER_NAME']
        file_label = instance_name or env_name

        print(f"Activating {file_label} container (env: {env_name})...")

        # Start SSH agent and add keys
        # If we're inside a container with an existing SSH agent socket (e.g. docker-dev),
        # reuse it instead of starting a new one. A new agent's socket would live in the
        # container's filesystem, invisible to the host Docker daemon for builds.
        existing_sock = os.environ.get('SSH_AUTH_SOCK', '')
        if existing_sock and Path(existing_sock).exists():
            pass  # Reuse the existing agent
        else:
            try:
                ssh_agent_output = subprocess.check_output(["ssh-agent", "-s"], text=True)
                # Parse and set SSH_AUTH_SOCK and SSH_AGENT_PID
                for line in ssh_agent_output.split('\n'):
                    if 'SSH_AUTH_SOCK' in line or 'SSH_AGENT_PID' in line:
                        parts = line.split(';')[0].split('=')
                        if len(parts) == 2:
                            os.environ[parts[0]] = parts[1]

                # Add SSH key
                ssh_key = Path.home() / ".ssh" / "id_ed25519"
                if ssh_key.exists():
                    subprocess.run(["ssh-add", str(ssh_key)], check=False)
            except Exception as e:
                print(f"Warning: Failed to setup SSH agent: {e}")

        # Set environment variables for docker-compose
        # If HOST_SSH_AUTH_SOCK is set, we're inside a container (e.g. docker-dev) and
        # need to use the original host path so the host Docker daemon can find the socket.
        host_ssh_sock = os.environ.get('HOST_SSH_AUTH_SOCK', '')
        config['SSH_AUTH_SOCK'] = host_ssh_sock if host_ssh_sock else os.environ.get('SSH_AUTH_SOCK', '')
        config['DOCKERFILE_PATH'] = self.get_dockerfile_path(env_name, instance_name)
        config['CACHE_BUSTER'] = self._get_dotfiles_commit()

        # Inject host user info for non-root container user
        host_user = pwd.getpwuid(os.getuid())
        config.setdefault('HOST_USERNAME', host_user.pw_name)
        config.setdefault('HOST_UID', str(host_user.pw_uid))
        config.setdefault('HOST_GID', str(host_user.pw_gid))
        config.setdefault('USER_HOME', f"/home/{config['HOST_USERNAME']}")

        # Detect host Docker socket GID for Docker-in-Docker environments
        docker_sock = Path("/var/run/docker.sock")
        if docker_sock.exists():
            config.setdefault('DOCKER_GID', str(docker_sock.stat().st_gid))

        # Generate docker-compose file
        compose_file = self.generate_compose_file(env_name, config, instance_name)

        # Check container status and start if needed
        if self.container_is_running(container_name):
            print("Container is already running. Connecting...")
        elif self.container_exists(container_name):
            print("Starting existing container...")
            subprocess.run(["docker", "start", container_name], check=True)
        else:
            print("Creating and starting new container...")
            subprocess.run(
                ["docker", "compose", "-f", str(compose_file), "build"],
                cwd=self.project_root,
                check=True
            )
            subprocess.run(
                ["docker", "compose", "-f", str(compose_file), "-p", file_label, "up", "-d"],
                cwd=self.project_root,
                check=True
            )
            print("Container started successfully!")

        print("Entering container shell...")
        subprocess.run(["docker", "exec", "-it", "-u", config['HOST_USERNAME'], container_name, "zsh", "--login"])

    def cmd_deactivate(self, env_name: str, instance_name: Optional[str] = None):
        """Stop a container environment"""
        config = self.load_environment_config(env_name, instance_name)
        container_name = config['CONTAINER_NAME']

        print(f"Stopping {container_name}...")

        if not self.container_is_running(container_name):
            print("Container is not running.")
            return

        subprocess.run(["docker", "stop", container_name], check=True)
        print("Container stopped successfully!")

    def cmd_exec(self, env_name: str, command: Optional[list] = None, instance_name: Optional[str] = None):
        """Execute command in running container"""
        config = self.load_environment_config(env_name, instance_name)
        container_name = config['CONTAINER_NAME']

        if not self.container_is_running(container_name):
            print(f"Error: Container {container_name} is not running")
            name_hint = f" --name {instance_name}" if instance_name else ""
            print(f"Use 'container activate {env_name}{name_hint}' to start it")
            sys.exit(1)

        # Execute command or start shell
        cmd = command if command else ["zsh", "--login"]
        host_username = pwd.getpwuid(os.getuid()).pw_name
        subprocess.run(["docker", "exec", "-it", "-u", host_username, container_name] + cmd)

    def cmd_remove(self, env_name: str, instance_name: Optional[str] = None):
        """Remove container and cleanup"""
        file_label = instance_name or env_name
        config = self.load_environment_config(env_name, instance_name)
        container_name = config['CONTAINER_NAME']

        print(f"Removing container {container_name}...")

        # Stop container if running
        if self.container_is_running(container_name):
            subprocess.run(["docker", "stop", container_name], check=True)

        # Remove container if exists
        if self.container_exists(container_name):
            subprocess.run(["docker", "rm", container_name], check=True)

        # Remove generated compose file
        compose_file = self.project_root / f".docker-compose.{file_label}.yml"
        if compose_file.exists():
            compose_file.unlink()

        # Remove generated Dockerfile if any
        dockerfile = self.project_root / f".Dockerfile.{file_label}"
        if dockerfile.exists():
            dockerfile.unlink()

        print("Container removed successfully!")

    def _transform_for_remote(self, compose_path: Path, env_name: str, registry: str, tag: str, build_mode: bool = False):
        """Post-process a generated compose file for remote deployment.

        Pull mode (default): Replace image with registry path, remove build section.
        Build mode: Keep build section but rewrite paths for the remote host.
        Both modes remove the .:/host volume mount.
        """
        with open(compose_path) as f:
            data = yaml.safe_load(f)

        service = data['services']['dev-env']

        # Replace image with registry path
        service['image'] = f"{registry}/{env_name}:{tag}"

        if build_mode:
            # Keep build section but rewrite paths for the remote host
            build = service.get('build', {})
            build['context'] = '/home/midavis'
            build['dockerfile'] = f'/home/midavis/.Dockerfile.{env_name}'
            service['build'] = build
        else:
            # Pull mode: remove build section entirely
            service.pop('build', None)

        # Remove volumes that don't apply on the remote cluster:
        # - .:/host (nv-env project root isn't on the cluster)
        # - ~/.zprofile:* (Docker daemon can't access NFS home dirs; docker cp instead)
        # - *:/ssh-agent (SSH agent forwarding doesn't work across srun invocations;
        #   copy SSH keys into the container via docker cp instead)
        if 'volumes' in service:
            fixed_volumes = []
            for v in service['volumes']:
                if not isinstance(v, str):
                    fixed_volumes.append(v)
                    continue
                if v.startswith('.:/host'):
                    continue
                if v.startswith('~/.zprofile:'):
                    continue
                if v.endswith(':/ssh-agent'):
                    continue
                fixed_volumes.append(v)
            service['volumes'] = fixed_volumes

        # Remove SSH_AUTH_SOCK environment variable (no agent socket on cluster)
        if 'environment' in service:
            service['environment'] = [
                e for e in service['environment']
                if not (isinstance(e, str) and e.startswith('SSH_AUTH_SOCK='))
            ]

        with open(compose_path, 'w') as f:
            yaml.dump(data, f, default_flow_style=False, sort_keys=False)

    def cmd_generate(self, env_name: Optional[str] = None, all_envs: bool = False,
                     remote: bool = False, registry: str = 'gitlab-master.nvidia.com:5005/midavis/nv-env',
                     tag: str = 'latest', build_mode: bool = False):
        """Generate Dockerfile and compose file without building.
        Useful for CI pipelines that handle the build step separately.
        With --remote, transforms the compose file for remote deployment."""
        if all_envs:
            env_dir = self.config_dir / "environments"
            if not env_dir.exists():
                print(f"No environments found in {env_dir}")
                sys.exit(1)
            env_names = [p.name for p in sorted(env_dir.iterdir()) if p.is_dir() and (p / "config.env").exists()]
        elif env_name:
            env_names = [env_name]
        else:
            print("Error: provide an environment name or --all")
            sys.exit(1)

        for name in env_names:
            config = self.load_environment_config(name)

            # Provide CI-safe defaults for host-detected vars
            config.setdefault('HOST_USERNAME', 'dev')
            config.setdefault('HOST_UID', '1000')
            config.setdefault('HOST_GID', '1000')
            config.setdefault('USER_HOME', '/home/dev')
            config.setdefault('SSH_AUTH_SOCK', '')
            config.setdefault('CACHE_BUSTER', '1')
            config['DOCKERFILE_PATH'] = self.get_dockerfile_path(name)

            compose_path = self.generate_compose_file(name, config)
            dockerfile_path = config['DOCKERFILE_PATH']

            if remote:
                self._transform_for_remote(compose_path, name, registry, tag, build_mode)
                mode_label = "build" if build_mode else "pull"
                print(f"{name} (remote/{mode_label}):")
                print(f"  Compose file: {compose_path.relative_to(self.project_root)}")
                print(f"  Image:        {registry}/{name}:{tag}")
                if build_mode:
                    print(f"  Dockerfile:   {dockerfile_path}")
            else:
                print(f"{name}:")
                print(f"  Dockerfile:   {dockerfile_path}")
                print(f"  Compose file: {compose_path.relative_to(self.project_root)}")

    def cmd_build(self, env_name: str, push: bool = False, registry: Optional[str] = None, tags: Optional[list] = None):
        """Build a container image, optionally push to a registry."""
        config = self.load_environment_config(env_name)

        # Provide CI-safe defaults for host-detected vars
        config.setdefault('HOST_USERNAME', 'dev')
        config.setdefault('HOST_UID', '1000')
        config.setdefault('HOST_GID', '1000')
        config.setdefault('USER_HOME', '/home/dev')
        config.setdefault('SSH_AUTH_SOCK', '')
        config.setdefault('CACHE_BUSTER', self._get_dotfiles_commit())
        config['DOCKERFILE_PATH'] = self.get_dockerfile_path(env_name)

        # Generate compose file
        compose_file = self.generate_compose_file(env_name, config)

        print(f"Building {env_name}...")
        subprocess.run(
            ["docker", "compose", "-f", str(compose_file), "build"],
            cwd=self.project_root,
            check=True,
        )

        image_name = config['IMAGE_NAME']

        if push:
            if not registry:
                print("Error: --registry is required when using --push")
                sys.exit(1)
            if not tags:
                tags = ["latest"]

            for tag in tags:
                remote_tag = f"{registry}/{env_name}:{tag}"
                print(f"Tagging {image_name} -> {remote_tag}")
                subprocess.run(["docker", "tag", image_name, remote_tag], check=True)
                print(f"Pushing {remote_tag}...")
                subprocess.run(["docker", "push", remote_tag], check=True)

        print("Done.")

    def cmd_logs(self, env_name: str, follow: bool = False, tail: Optional[str] = None, instance_name: Optional[str] = None):
        """Show container logs"""
        config = self.load_environment_config(env_name, instance_name)
        container_name = config['CONTAINER_NAME']

        cmd = ["docker", "logs"]
        if follow:
            cmd.append("-f")
        if tail:
            cmd.extend(["--tail", tail])
        cmd.append(container_name)

        subprocess.run(cmd)

    def _srun_ssh(self, job_id: str, command: str, check: bool = True) -> subprocess.CompletedProcess:
        """Run a command on dlcluster via srun against a Slurm job allocation."""
        return subprocess.run(
            ["ssh", "midavis@dlcluster", f"srun --jobid={job_id} {command}"],
            check=check
        )

    def cmd_deploy(self, env_name: str, job_id: str, tag: str = 'latest',
                   registry: str = 'gitlab-master.nvidia.com:5005/midavis/nv-env',
                   skip_setup: bool = False):
        """Deploy a container to dlcluster: SCP compose, pull, start, inject files."""
        config = self.load_environment_config(env_name)
        container_name = config['CONTAINER_NAME']
        compose_filename = f".docker-compose.{env_name}.yml"
        local_compose = self.project_root / compose_filename
        remote_compose = f"/home/midavis/{compose_filename}"

        if not local_compose.exists():
            print(f"Error: {compose_filename} not found. Run 'container generate {env_name} --remote' first.")
            sys.exit(1)

        # 1. SCP compose file to cluster
        print(f"Copying {compose_filename} to dlcluster...")
        subprocess.run(["scp", str(local_compose), f"midavis@dlcluster:/home/midavis/"], check=True)

        # 2. One-time setup (idempotent â€” volume create is a no-op if it exists)
        if not skip_setup:
            print("Ensuring dev-vol exists...")
            self._srun_ssh(job_id, "docker volume create dev-vol", check=False)

        # 3. Pull and start
        print(f"Pulling {registry}/{env_name}:{tag}...")
        self._srun_ssh(job_id, f"docker compose -f {remote_compose} pull")
        print("Starting container...")
        self._srun_ssh(job_id, f"docker compose -f {remote_compose} up -d")

        # 4. Inject SSH key via docker cp through /tmp
        print("Injecting SSH key...")
        self._srun_ssh(job_id,
            f"bash -c 'cp ~/.ssh/id_ed25519 /tmp/id_ed25519 && "
            f"docker cp /tmp/id_ed25519 {container_name}:/home/dev/.ssh/id_ed25519 && "
            f"rm /tmp/id_ed25519'")
        self._srun_ssh(job_id,
            f"docker exec -u root {container_name} bash -c "
            f"'chown 1000:1000 /home/dev/.ssh/id_ed25519 && chmod 600 /home/dev/.ssh/id_ed25519'")

        # 5. Inject .zprofile via docker cp through /tmp
        print("Injecting .zprofile...")
        self._srun_ssh(job_id,
            f"bash -c 'cp ~/.zprofile /tmp/.zprofile && "
            f"docker cp /tmp/.zprofile {container_name}:/home/dev/.zprofile && "
            f"rm /tmp/.zprofile'")

        print(f"\nContainer {container_name} is running on job {job_id}.")
        print(f"  Exec:    ssh midavis@dlcluster \"srun --jobid={job_id} docker exec {container_name} <cmd>\"")
        print(f"  Shell:   tmux new-window -n dlcluster \\; send-keys "
              f"\"ssh -t midavis@dlcluster 'srun --pty --jobid={job_id} docker exec -it {container_name} zsh'\" Enter")


def main():
    manager = ContainerManager()

    parser = argparse.ArgumentParser(
        description="Container Management CLI for dev-containers framework",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    subparsers = parser.add_subparsers(dest='command', help='Available commands')

    # list command
    subparsers.add_parser('list', help='List all available container environments')

    # activate command
    activate_parser = subparsers.add_parser('activate', help='Start and attach to a container environment')
    activate_parser.add_argument('env_name', help='Environment name')
    activate_parser.add_argument('--name', dest='instance_name', help='Custom instance name (creates a separate container from the same environment)')

    # deactivate command
    deactivate_parser = subparsers.add_parser('deactivate', help='Stop a container environment')
    deactivate_parser.add_argument('env_name', help='Environment name')
    deactivate_parser.add_argument('--name', dest='instance_name', help='Custom instance name')

    # exec command
    exec_parser = subparsers.add_parser('exec', help='Execute command in running container')
    exec_parser.add_argument('env_name', help='Environment name')
    exec_parser.add_argument('--name', dest='instance_name', help='Custom instance name')
    exec_parser.add_argument('command', nargs='*', help='Command to execute (default: zsh)')

    # remove command
    remove_parser = subparsers.add_parser('remove', help='Remove container and cleanup')
    remove_parser.add_argument('env_name', help='Environment name')
    remove_parser.add_argument('--name', dest='instance_name', help='Custom instance name')

    # generate command
    generate_parser = subparsers.add_parser('generate', help='Generate Dockerfile and compose file without building')
    generate_parser.add_argument('env_name', nargs='?', help='Environment name')
    generate_parser.add_argument('--all', dest='all_envs', action='store_true', help='Generate for all environments')
    generate_parser.add_argument('--remote', action='store_true', help='Transform compose file for remote deployment (pull mode by default)')
    generate_parser.add_argument('--registry', default='gitlab-master.nvidia.com:5005/midavis/nv-env', help='Container registry URL (default: gitlab-master.nvidia.com:5005/midavis/nv-env)')
    generate_parser.add_argument('--tag', default='latest', help='Image tag (default: latest)')
    generate_parser.add_argument('--build', dest='build_mode', action='store_true', help='Remote build mode: keep build section with rewritten paths')

    # build command
    build_parser = subparsers.add_parser('build', help='Build a container image')
    build_parser.add_argument('env_name', help='Environment name')
    build_parser.add_argument('--push', action='store_true', help='Push image to registry after building')
    build_parser.add_argument('--registry', help='Registry URL (required with --push)')
    build_parser.add_argument('--tag', dest='tags', action='append', help='Image tag (can be specified multiple times, default: latest)')

    # logs command
    logs_parser = subparsers.add_parser('logs', help='Show container logs')
    logs_parser.add_argument('env_name', help='Environment name')
    logs_parser.add_argument('--name', dest='instance_name', help='Custom instance name')
    logs_parser.add_argument('-f', '--follow', action='store_true', help='Follow log output')
    logs_parser.add_argument('--tail', help='Number of lines to show from the end')

    # deploy command
    deploy_parser = subparsers.add_parser('deploy', help='Deploy container to dlcluster via Slurm job')
    deploy_parser.add_argument('env_name', help='Environment name')
    deploy_parser.add_argument('--job-id', required=True, help='Slurm job ID from sbatch')
    deploy_parser.add_argument('--tag', default='latest', help='Image tag (default: latest)')
    deploy_parser.add_argument('--registry', default='gitlab-master.nvidia.com:5005/midavis/nv-env')
    deploy_parser.add_argument('--skip-setup', action='store_true', help='Skip volume creation')

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        sys.exit(1)

    # Execute command
    try:
        if args.command == 'list':
            manager.cmd_list()
        elif args.command == 'activate':
            manager.cmd_activate(args.env_name, args.instance_name)
        elif args.command == 'deactivate':
            manager.cmd_deactivate(args.env_name, args.instance_name)
        elif args.command == 'exec':
            manager.cmd_exec(args.env_name, args.command if args.command else None, args.instance_name)
        elif args.command == 'remove':
            manager.cmd_remove(args.env_name, args.instance_name)
        elif args.command == 'generate':
            manager.cmd_generate(args.env_name, args.all_envs, args.remote, args.registry, args.tag, args.build_mode)
        elif args.command == 'build':
            manager.cmd_build(args.env_name, args.push, args.registry, args.tags)
        elif args.command == 'logs':
            manager.cmd_logs(args.env_name, args.follow, args.tail, args.instance_name)
        elif args.command == 'deploy':
            manager.cmd_deploy(args.env_name, args.job_id, args.tag, args.registry, args.skip_setup)
    except KeyboardInterrupt:
        print("\nInterrupted by user")
        sys.exit(130)
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == '__main__':
    main()
